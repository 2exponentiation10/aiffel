{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0612a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "#sklearn.datasets의 load_diabetes에서 데이터호출\n",
    "di = load_diabetes()\n",
    "\n",
    "df_X=di.data\n",
    "df_y=di.target\n",
    "#diabetes의 data를 df_X에, target을 df_y에 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1a97d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1 : age\n",
      "feature 2 : sex\n",
      "feature 3 : bmi\n",
      "feature 4 : bp\n",
      "feature 5 : s1\n",
      "feature 6 : s2\n",
      "feature 7 : s3\n",
      "feature 8 : s4\n",
      "feature 9 : s5\n",
      "feature 10 : s6\n"
     ]
    }
   ],
   "source": [
    "for i,feature_name in enumerate(di.feature_names):\n",
    "  print(f'feature {i+1} : {feature_name}')\n",
    "#특성확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7405b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<df_X[0]> :  [ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n",
      " -0.04340085 -0.00259226  0.01990842 -0.01764613]\n",
      "\n",
      "<df_y[0]> :  151.0\n"
     ]
    }
   ],
   "source": [
    "print('<df_X[0]> : ',df_X[0])\n",
    "print()\n",
    "print('<df_y[0]> : ',df_y[0] )\n",
    "\n",
    "#환자의 데이터확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96466f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_X= np.array(df_X)\n",
    "df_y= np.array(df_y)\n",
    "\n",
    "#넘파이 배열이겠지만 그래도...변환!\n",
    "\n",
    "print(df_X.shape)\n",
    "print(df_y.shape)\n",
    "\n",
    "#각각의 쉐입 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56438683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "#sklearn의 내장된 train_test_split호출\n",
    "#8:2의 비율로 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c162fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169f1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "#환자의 특성이 10개 이기에 w에는 10개의 랜덤상수,b에는 한개의 랜덤 상수가 들어가있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b65f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델생성!!\n",
    "def model(X, W, b):\n",
    "    pre = 0\n",
    "    for i in range(9):\n",
    "        pre += X[:, i] * W[i]\n",
    "    pre += b\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab53c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수로 실제값과 예상값의 차를 제곱평균인 MSE로 설정\n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    pre = model(X, W, b)\n",
    "    L = MSE(pre, y)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76c0401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#기울기 계산함수\n",
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cdaa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_rate=0.9\n",
    "#0.9가 다양한 값을 테스트했을때 베스트로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6131ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 : Loss 27666.4818\n",
      "Iteration 2 : Loss 25901.1551\n",
      "Iteration 3 : Loss 24270.7168\n",
      "Iteration 4 : Loss 22764.6853\n",
      "Iteration 5 : Loss 21373.3979\n",
      "Iteration 6 : Loss 20087.9463\n",
      "Iteration 7 : Loss 18900.1181\n",
      "Iteration 8 : Loss 17802.3418\n",
      "Iteration 9 : Loss 16787.6371\n",
      "Iteration 10 : Loss 15849.5685\n",
      "Iteration 11 : Loss 14982.2026\n",
      "Iteration 12 : Loss 14180.0687\n",
      "Iteration 13 : Loss 13438.1230\n",
      "Iteration 14 : Loss 12751.7149\n",
      "Iteration 15 : Loss 12116.5563\n",
      "Iteration 16 : Loss 11528.6933\n",
      "Iteration 17 : Loss 10984.4801\n",
      "Iteration 18 : Loss 10480.5547\n",
      "Iteration 19 : Loss 10013.8171\n",
      "Iteration 20 : Loss 9581.4084\n",
      "Iteration 21 : Loss 9180.6923\n",
      "Iteration 22 : Loss 8809.2374\n",
      "Iteration 23 : Loss 8464.8015\n",
      "Iteration 24 : Loss 8145.3166\n",
      "Iteration 25 : Loss 7848.8753\n",
      "Iteration 26 : Loss 7573.7185\n",
      "Iteration 27 : Loss 7318.2234\n",
      "Iteration 28 : Loss 7080.8933\n",
      "Iteration 29 : Loss 6860.3474\n",
      "Iteration 30 : Loss 6655.3120\n",
      "Iteration 31 : Loss 6464.6120\n",
      "Iteration 32 : Loss 6287.1633\n",
      "Iteration 33 : Loss 6121.9657\n",
      "Iteration 34 : Loss 5968.0962\n",
      "Iteration 35 : Loss 5824.7032\n",
      "Iteration 36 : Loss 5691.0008\n",
      "Iteration 37 : Loss 5566.2638\n",
      "Iteration 38 : Loss 5449.8228\n",
      "Iteration 39 : Loss 5341.0600\n",
      "Iteration 40 : Loss 5239.4051\n",
      "Iteration 41 : Loss 5144.3316\n",
      "Iteration 42 : Loss 5055.3536\n",
      "Iteration 43 : Loss 4972.0224\n",
      "Iteration 44 : Loss 4893.9235\n",
      "Iteration 45 : Loss 4820.6743\n",
      "Iteration 46 : Loss 4751.9215\n",
      "Iteration 47 : Loss 4687.3386\n",
      "Iteration 48 : Loss 4626.6242\n",
      "Iteration 49 : Loss 4569.4997\n",
      "Iteration 50 : Loss 4515.7078\n",
      "Iteration 51 : Loss 4465.0105\n",
      "Iteration 52 : Loss 4417.1881\n",
      "Iteration 53 : Loss 4372.0375\n",
      "Iteration 54 : Loss 4329.3707\n",
      "Iteration 55 : Loss 4289.0142\n",
      "Iteration 56 : Loss 4250.8073\n",
      "Iteration 57 : Loss 4214.6014\n",
      "Iteration 58 : Loss 4180.2593\n",
      "Iteration 59 : Loss 4147.6537\n",
      "Iteration 60 : Loss 4116.6669\n",
      "Iteration 61 : Loss 4087.1902\n",
      "Iteration 62 : Loss 4059.1226\n",
      "Iteration 63 : Loss 4032.3708\n",
      "Iteration 64 : Loss 4006.8483\n",
      "Iteration 65 : Loss 3982.4751\n",
      "Iteration 66 : Loss 3959.1768\n",
      "Iteration 67 : Loss 3936.8847\n",
      "Iteration 68 : Loss 3915.5350\n",
      "Iteration 69 : Loss 3895.0684\n",
      "Iteration 70 : Loss 3875.4302\n",
      "Iteration 71 : Loss 3856.5694\n",
      "Iteration 72 : Loss 3838.4387\n",
      "Iteration 73 : Loss 3820.9942\n",
      "Iteration 74 : Loss 3804.1950\n",
      "Iteration 75 : Loss 3788.0035\n",
      "Iteration 76 : Loss 3772.3841\n",
      "Iteration 77 : Loss 3757.3044\n",
      "Iteration 78 : Loss 3742.7336\n",
      "Iteration 79 : Loss 3728.6436\n",
      "Iteration 80 : Loss 3715.0079\n",
      "Iteration 81 : Loss 3701.8018\n",
      "Iteration 82 : Loss 3689.0024\n",
      "Iteration 83 : Loss 3676.5884\n",
      "Iteration 84 : Loss 3664.5399\n",
      "Iteration 85 : Loss 3652.8381\n",
      "Iteration 86 : Loss 3641.4658\n",
      "Iteration 87 : Loss 3630.4067\n",
      "Iteration 88 : Loss 3619.6456\n",
      "Iteration 89 : Loss 3609.1683\n",
      "Iteration 90 : Loss 3598.9616\n",
      "Iteration 91 : Loss 3589.0129\n",
      "Iteration 92 : Loss 3579.3106\n",
      "Iteration 93 : Loss 3569.8438\n",
      "Iteration 94 : Loss 3560.6023\n",
      "Iteration 95 : Loss 3551.5763\n",
      "Iteration 96 : Loss 3542.7569\n",
      "Iteration 97 : Loss 3534.1356\n",
      "Iteration 98 : Loss 3525.7042\n",
      "Iteration 99 : Loss 3517.4554\n",
      "Iteration 100 : Loss 3509.3820\n",
      "Iteration 101 : Loss 3501.4773\n",
      "Iteration 102 : Loss 3493.7350\n",
      "Iteration 103 : Loss 3486.1492\n",
      "Iteration 104 : Loss 3478.7141\n",
      "Iteration 105 : Loss 3471.4246\n",
      "Iteration 106 : Loss 3464.2755\n",
      "Iteration 107 : Loss 3457.2621\n",
      "Iteration 108 : Loss 3450.3798\n",
      "Iteration 109 : Loss 3443.6245\n",
      "Iteration 110 : Loss 3436.9919\n",
      "Iteration 111 : Loss 3430.4782\n",
      "Iteration 112 : Loss 3424.0798\n",
      "Iteration 113 : Loss 3417.7931\n",
      "Iteration 114 : Loss 3411.6149\n",
      "Iteration 115 : Loss 3405.5419\n",
      "Iteration 116 : Loss 3399.5711\n",
      "Iteration 117 : Loss 3393.6997\n",
      "Iteration 118 : Loss 3387.9248\n",
      "Iteration 119 : Loss 3382.2439\n",
      "Iteration 120 : Loss 3376.6544\n",
      "Iteration 121 : Loss 3371.1539\n",
      "Iteration 122 : Loss 3365.7401\n",
      "Iteration 123 : Loss 3360.4108\n",
      "Iteration 124 : Loss 3355.1639\n",
      "Iteration 125 : Loss 3349.9973\n",
      "Iteration 126 : Loss 3344.9090\n",
      "Iteration 127 : Loss 3339.8972\n",
      "Iteration 128 : Loss 3334.9601\n",
      "Iteration 129 : Loss 3330.0959\n",
      "Iteration 130 : Loss 3325.3030\n",
      "Iteration 131 : Loss 3320.5797\n",
      "Iteration 132 : Loss 3315.9245\n",
      "Iteration 133 : Loss 3311.3359\n",
      "Iteration 134 : Loss 3306.8124\n",
      "Iteration 135 : Loss 3302.3527\n",
      "Iteration 136 : Loss 3297.9553\n",
      "Iteration 137 : Loss 3293.6190\n",
      "Iteration 138 : Loss 3289.3425\n",
      "Iteration 139 : Loss 3285.1246\n",
      "Iteration 140 : Loss 3280.9642\n",
      "Iteration 141 : Loss 3276.8599\n",
      "Iteration 142 : Loss 3272.8108\n",
      "Iteration 143 : Loss 3268.8158\n",
      "Iteration 144 : Loss 3264.8738\n",
      "Iteration 145 : Loss 3260.9838\n",
      "Iteration 146 : Loss 3257.1449\n",
      "Iteration 147 : Loss 3253.3560\n",
      "Iteration 148 : Loss 3249.6163\n",
      "Iteration 149 : Loss 3245.9248\n",
      "Iteration 150 : Loss 3242.2806\n",
      "Iteration 151 : Loss 3238.6830\n",
      "Iteration 152 : Loss 3235.1310\n",
      "Iteration 153 : Loss 3231.6240\n",
      "Iteration 154 : Loss 3228.1610\n",
      "Iteration 155 : Loss 3224.7413\n",
      "Iteration 156 : Loss 3221.3642\n",
      "Iteration 157 : Loss 3218.0290\n",
      "Iteration 158 : Loss 3214.7349\n",
      "Iteration 159 : Loss 3211.4813\n",
      "Iteration 160 : Loss 3208.2674\n",
      "Iteration 161 : Loss 3205.0927\n",
      "Iteration 162 : Loss 3201.9565\n",
      "Iteration 163 : Loss 3198.8581\n",
      "Iteration 164 : Loss 3195.7969\n",
      "Iteration 165 : Loss 3192.7724\n",
      "Iteration 166 : Loss 3189.7840\n",
      "Iteration 167 : Loss 3186.8311\n",
      "Iteration 168 : Loss 3183.9132\n",
      "Iteration 169 : Loss 3181.0296\n",
      "Iteration 170 : Loss 3178.1799\n",
      "Iteration 171 : Loss 3175.3636\n",
      "Iteration 172 : Loss 3172.5801\n",
      "Iteration 173 : Loss 3169.8289\n",
      "Iteration 174 : Loss 3167.1097\n",
      "Iteration 175 : Loss 3164.4218\n",
      "Iteration 176 : Loss 3161.7648\n",
      "Iteration 177 : Loss 3159.1383\n",
      "Iteration 178 : Loss 3156.5419\n",
      "Iteration 179 : Loss 3153.9750\n",
      "Iteration 180 : Loss 3151.4373\n",
      "Iteration 181 : Loss 3148.9283\n",
      "Iteration 182 : Loss 3146.4477\n",
      "Iteration 183 : Loss 3143.9949\n",
      "Iteration 184 : Loss 3141.5698\n",
      "Iteration 185 : Loss 3139.1717\n",
      "Iteration 186 : Loss 3136.8005\n",
      "Iteration 187 : Loss 3134.4556\n",
      "Iteration 188 : Loss 3132.1368\n",
      "Iteration 189 : Loss 3129.8436\n",
      "Iteration 190 : Loss 3127.5758\n",
      "Iteration 191 : Loss 3125.3329\n",
      "Iteration 192 : Loss 3123.1147\n",
      "Iteration 193 : Loss 3120.9208\n",
      "Iteration 194 : Loss 3118.7509\n",
      "Iteration 195 : Loss 3116.6047\n",
      "Iteration 196 : Loss 3114.4818\n",
      "Iteration 197 : Loss 3112.3820\n",
      "Iteration 198 : Loss 3110.3050\n",
      "Iteration 199 : Loss 3108.2504\n",
      "Iteration 200 : Loss 3106.2179\n",
      "Iteration 201 : Loss 3104.2073\n",
      "Iteration 202 : Loss 3102.2183\n",
      "Iteration 203 : Loss 3100.2507\n",
      "Iteration 204 : Loss 3098.3040\n",
      "Iteration 205 : Loss 3096.3782\n",
      "Iteration 206 : Loss 3094.4729\n",
      "Iteration 207 : Loss 3092.5878\n",
      "Iteration 208 : Loss 3090.7228\n",
      "Iteration 209 : Loss 3088.8774\n",
      "Iteration 210 : Loss 3087.0517\n",
      "Iteration 211 : Loss 3085.2451\n",
      "Iteration 212 : Loss 3083.4576\n",
      "Iteration 213 : Loss 3081.6889\n",
      "Iteration 214 : Loss 3079.9388\n",
      "Iteration 215 : Loss 3078.2070\n",
      "Iteration 216 : Loss 3076.4934\n",
      "Iteration 217 : Loss 3074.7976\n",
      "Iteration 218 : Loss 3073.1195\n",
      "Iteration 219 : Loss 3071.4589\n",
      "Iteration 220 : Loss 3069.8156\n",
      "Iteration 221 : Loss 3068.1893\n",
      "Iteration 222 : Loss 3066.5799\n",
      "Iteration 223 : Loss 3064.9871\n",
      "Iteration 224 : Loss 3063.4108\n",
      "Iteration 225 : Loss 3061.8508\n",
      "Iteration 226 : Loss 3060.3069\n",
      "Iteration 227 : Loss 3058.7788\n",
      "Iteration 228 : Loss 3057.2664\n",
      "Iteration 229 : Loss 3055.7696\n",
      "Iteration 230 : Loss 3054.2881\n",
      "Iteration 231 : Loss 3052.8218\n",
      "Iteration 232 : Loss 3051.3705\n",
      "Iteration 233 : Loss 3049.9340\n",
      "Iteration 234 : Loss 3048.5121\n",
      "Iteration 235 : Loss 3047.1048\n",
      "Iteration 236 : Loss 3045.7117\n",
      "Iteration 237 : Loss 3044.3328\n",
      "Iteration 238 : Loss 3042.9679\n",
      "Iteration 239 : Loss 3041.6169\n",
      "Iteration 240 : Loss 3040.2795\n",
      "Iteration 241 : Loss 3038.9557\n",
      "Iteration 242 : Loss 3037.6453\n",
      "Iteration 243 : Loss 3036.3481\n",
      "Iteration 244 : Loss 3035.0639\n",
      "Iteration 245 : Loss 3033.7927\n",
      "Iteration 246 : Loss 3032.5344\n",
      "Iteration 247 : Loss 3031.2886\n",
      "Iteration 248 : Loss 3030.0554\n",
      "Iteration 249 : Loss 3028.8346\n",
      "Iteration 250 : Loss 3027.6260\n",
      "Iteration 251 : Loss 3026.4295\n",
      "Iteration 252 : Loss 3025.2450\n",
      "Iteration 253 : Loss 3024.0723\n",
      "Iteration 254 : Loss 3022.9114\n",
      "Iteration 255 : Loss 3021.7620\n",
      "Iteration 256 : Loss 3020.6241\n",
      "Iteration 257 : Loss 3019.4976\n",
      "Iteration 258 : Loss 3018.3822\n",
      "Iteration 259 : Loss 3017.2780\n",
      "Iteration 260 : Loss 3016.1847\n",
      "Iteration 261 : Loss 3015.1024\n",
      "Iteration 262 : Loss 3014.0307\n",
      "Iteration 263 : Loss 3012.9697\n",
      "Iteration 264 : Loss 3011.9192\n",
      "Iteration 265 : Loss 3010.8791\n",
      "Iteration 266 : Loss 3009.8493\n",
      "Iteration 267 : Loss 3008.8297\n",
      "Iteration 268 : Loss 3007.8202\n",
      "Iteration 269 : Loss 3006.8206\n",
      "Iteration 270 : Loss 3005.8309\n",
      "Iteration 271 : Loss 3004.8510\n",
      "Iteration 272 : Loss 3003.8807\n",
      "Iteration 273 : Loss 3002.9200\n",
      "Iteration 274 : Loss 3001.9687\n",
      "Iteration 275 : Loss 3001.0268\n",
      "Iteration 276 : Loss 3000.0942\n",
      "Iteration 277 : Loss 2999.1707\n",
      "Iteration 278 : Loss 2998.2563\n",
      "Iteration 279 : Loss 2997.3509\n",
      "Iteration 280 : Loss 2996.4543\n",
      "Iteration 281 : Loss 2995.5665\n",
      "Iteration 282 : Loss 2994.6875\n",
      "Iteration 283 : Loss 2993.8170\n",
      "Iteration 284 : Loss 2992.9551\n",
      "Iteration 285 : Loss 2992.1016\n",
      "Iteration 286 : Loss 2991.2564\n",
      "Iteration 287 : Loss 2990.4195\n",
      "Iteration 288 : Loss 2989.5908\n",
      "Iteration 289 : Loss 2988.7701\n",
      "Iteration 290 : Loss 2987.9575\n",
      "Iteration 291 : Loss 2987.1527\n",
      "Iteration 292 : Loss 2986.3559\n",
      "Iteration 293 : Loss 2985.5667\n",
      "Iteration 294 : Loss 2984.7853\n",
      "Iteration 295 : Loss 2984.0115\n",
      "Iteration 296 : Loss 2983.2452\n",
      "Iteration 297 : Loss 2982.4863\n",
      "Iteration 298 : Loss 2981.7348\n",
      "Iteration 299 : Loss 2980.9906\n",
      "Iteration 300 : Loss 2980.2537\n",
      "Iteration 301 : Loss 2979.5239\n",
      "Iteration 302 : Loss 2978.8011\n",
      "Iteration 303 : Loss 2978.0854\n",
      "Iteration 304 : Loss 2977.3766\n",
      "Iteration 305 : Loss 2976.6747\n",
      "Iteration 306 : Loss 2975.9796\n",
      "Iteration 307 : Loss 2975.2912\n",
      "Iteration 308 : Loss 2974.6094\n",
      "Iteration 309 : Loss 2973.9343\n",
      "Iteration 310 : Loss 2973.2656\n",
      "Iteration 311 : Loss 2972.6035\n",
      "Iteration 312 : Loss 2971.9477\n",
      "Iteration 313 : Loss 2971.2983\n",
      "Iteration 314 : Loss 2970.6551\n",
      "Iteration 315 : Loss 2970.0181\n",
      "Iteration 316 : Loss 2969.3873\n",
      "Iteration 317 : Loss 2968.7626\n",
      "Iteration 318 : Loss 2968.1439\n",
      "Iteration 319 : Loss 2967.5311\n",
      "Iteration 320 : Loss 2966.9242\n",
      "Iteration 321 : Loss 2966.3232\n",
      "Iteration 322 : Loss 2965.7280\n",
      "Iteration 323 : Loss 2965.1385\n",
      "Iteration 324 : Loss 2964.5546\n",
      "Iteration 325 : Loss 2963.9764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 326 : Loss 2963.4037\n",
      "Iteration 327 : Loss 2962.8365\n",
      "Iteration 328 : Loss 2962.2748\n",
      "Iteration 329 : Loss 2961.7185\n",
      "Iteration 330 : Loss 2961.1675\n",
      "Iteration 331 : Loss 2960.6218\n",
      "Iteration 332 : Loss 2960.0813\n",
      "Iteration 333 : Loss 2959.5460\n",
      "Iteration 334 : Loss 2959.0158\n",
      "Iteration 335 : Loss 2958.4907\n",
      "Iteration 336 : Loss 2957.9706\n",
      "Iteration 337 : Loss 2957.4555\n",
      "Iteration 338 : Loss 2956.9454\n",
      "Iteration 339 : Loss 2956.4401\n",
      "Iteration 340 : Loss 2955.9397\n",
      "Iteration 341 : Loss 2955.4440\n",
      "Iteration 342 : Loss 2954.9531\n",
      "Iteration 343 : Loss 2954.4668\n",
      "Iteration 344 : Loss 2953.9852\n",
      "Iteration 345 : Loss 2953.5083\n",
      "Iteration 346 : Loss 2953.0358\n",
      "Iteration 347 : Loss 2952.5679\n",
      "Iteration 348 : Loss 2952.1044\n",
      "Iteration 349 : Loss 2951.6454\n",
      "Iteration 350 : Loss 2951.1907\n",
      "Iteration 351 : Loss 2950.7404\n",
      "Iteration 352 : Loss 2950.2943\n",
      "Iteration 353 : Loss 2949.8525\n",
      "Iteration 354 : Loss 2949.4149\n",
      "Iteration 355 : Loss 2948.9815\n",
      "Iteration 356 : Loss 2948.5522\n",
      "Iteration 357 : Loss 2948.1270\n",
      "Iteration 358 : Loss 2947.7058\n",
      "Iteration 359 : Loss 2947.2887\n",
      "Iteration 360 : Loss 2946.8755\n",
      "Iteration 361 : Loss 2946.4662\n",
      "Iteration 362 : Loss 2946.0608\n",
      "Iteration 363 : Loss 2945.6593\n",
      "Iteration 364 : Loss 2945.2615\n",
      "Iteration 365 : Loss 2944.8676\n",
      "Iteration 366 : Loss 2944.4774\n",
      "Iteration 367 : Loss 2944.0909\n",
      "Iteration 368 : Loss 2943.7080\n",
      "Iteration 369 : Loss 2943.3288\n",
      "Iteration 370 : Loss 2942.9532\n",
      "Iteration 371 : Loss 2942.5812\n",
      "Iteration 372 : Loss 2942.2126\n",
      "Iteration 373 : Loss 2941.8476\n",
      "Iteration 374 : Loss 2941.4860\n",
      "Iteration 375 : Loss 2941.1278\n",
      "Iteration 376 : Loss 2940.7731\n",
      "Iteration 377 : Loss 2940.4217\n",
      "Iteration 378 : Loss 2940.0736\n",
      "Iteration 379 : Loss 2939.7288\n",
      "Iteration 380 : Loss 2939.3872\n",
      "Iteration 381 : Loss 2939.0489\n",
      "Iteration 382 : Loss 2938.7138\n",
      "Iteration 383 : Loss 2938.3819\n",
      "Iteration 384 : Loss 2938.0530\n",
      "Iteration 385 : Loss 2937.7273\n",
      "Iteration 386 : Loss 2937.4047\n",
      "Iteration 387 : Loss 2937.0851\n",
      "Iteration 388 : Loss 2936.7685\n",
      "Iteration 389 : Loss 2936.4549\n",
      "Iteration 390 : Loss 2936.1443\n",
      "Iteration 391 : Loss 2935.8366\n",
      "Iteration 392 : Loss 2935.5318\n",
      "Iteration 393 : Loss 2935.2299\n",
      "Iteration 394 : Loss 2934.9308\n",
      "Iteration 395 : Loss 2934.6345\n",
      "Iteration 396 : Loss 2934.3410\n",
      "Iteration 397 : Loss 2934.0503\n",
      "Iteration 398 : Loss 2933.7623\n",
      "Iteration 399 : Loss 2933.4770\n",
      "Iteration 400 : Loss 2933.1944\n",
      "Iteration 401 : Loss 2932.9145\n",
      "Iteration 402 : Loss 2932.6372\n",
      "Iteration 403 : Loss 2932.3625\n",
      "Iteration 404 : Loss 2932.0903\n",
      "Iteration 405 : Loss 2931.8208\n",
      "Iteration 406 : Loss 2931.5537\n",
      "Iteration 407 : Loss 2931.2892\n",
      "Iteration 408 : Loss 2931.0271\n",
      "Iteration 409 : Loss 2930.7675\n",
      "Iteration 410 : Loss 2930.5104\n",
      "Iteration 411 : Loss 2930.2556\n",
      "Iteration 412 : Loss 2930.0033\n",
      "Iteration 413 : Loss 2929.7533\n",
      "Iteration 414 : Loss 2929.5056\n",
      "Iteration 415 : Loss 2929.2603\n",
      "Iteration 416 : Loss 2929.0173\n",
      "Iteration 417 : Loss 2928.7765\n",
      "Iteration 418 : Loss 2928.5380\n",
      "Iteration 419 : Loss 2928.3017\n",
      "Iteration 420 : Loss 2928.0676\n",
      "Iteration 421 : Loss 2927.8358\n",
      "Iteration 422 : Loss 2927.6060\n",
      "Iteration 423 : Loss 2927.3785\n",
      "Iteration 424 : Loss 2927.1530\n",
      "Iteration 425 : Loss 2926.9297\n",
      "Iteration 426 : Loss 2926.7085\n",
      "Iteration 427 : Loss 2926.4893\n",
      "Iteration 428 : Loss 2926.2721\n",
      "Iteration 429 : Loss 2926.0570\n",
      "Iteration 430 : Loss 2925.8439\n",
      "Iteration 431 : Loss 2925.6328\n",
      "Iteration 432 : Loss 2925.4237\n",
      "Iteration 433 : Loss 2925.2165\n",
      "Iteration 434 : Loss 2925.0112\n",
      "Iteration 435 : Loss 2924.8078\n",
      "Iteration 436 : Loss 2924.6064\n",
      "Iteration 437 : Loss 2924.4068\n",
      "Iteration 438 : Loss 2924.2090\n",
      "Iteration 439 : Loss 2924.0131\n",
      "Iteration 440 : Loss 2923.8191\n",
      "Iteration 441 : Loss 2923.6268\n",
      "Iteration 442 : Loss 2923.4363\n",
      "Iteration 443 : Loss 2923.2476\n",
      "Iteration 444 : Loss 2923.0607\n",
      "Iteration 445 : Loss 2922.8754\n",
      "Iteration 446 : Loss 2922.6919\n",
      "Iteration 447 : Loss 2922.5102\n",
      "Iteration 448 : Loss 2922.3300\n",
      "Iteration 449 : Loss 2922.1516\n",
      "Iteration 450 : Loss 2921.9748\n",
      "Iteration 451 : Loss 2921.7997\n",
      "Iteration 452 : Loss 2921.6262\n",
      "Iteration 453 : Loss 2921.4542\n",
      "Iteration 454 : Loss 2921.2839\n",
      "Iteration 455 : Loss 2921.1152\n",
      "Iteration 456 : Loss 2920.9480\n",
      "Iteration 457 : Loss 2920.7824\n",
      "Iteration 458 : Loss 2920.6183\n",
      "Iteration 459 : Loss 2920.4557\n",
      "Iteration 460 : Loss 2920.2946\n",
      "Iteration 461 : Loss 2920.1350\n",
      "Iteration 462 : Loss 2919.9769\n",
      "Iteration 463 : Loss 2919.8202\n",
      "Iteration 464 : Loss 2919.6650\n",
      "Iteration 465 : Loss 2919.5113\n",
      "Iteration 466 : Loss 2919.3589\n",
      "Iteration 467 : Loss 2919.2079\n",
      "Iteration 468 : Loss 2919.0584\n",
      "Iteration 469 : Loss 2918.9102\n",
      "Iteration 470 : Loss 2918.7634\n",
      "Iteration 471 : Loss 2918.6179\n",
      "Iteration 472 : Loss 2918.4738\n",
      "Iteration 473 : Loss 2918.3310\n",
      "Iteration 474 : Loss 2918.1896\n",
      "Iteration 475 : Loss 2918.0494\n",
      "Iteration 476 : Loss 2917.9105\n",
      "Iteration 477 : Loss 2917.7729\n",
      "Iteration 478 : Loss 2917.6365\n",
      "Iteration 479 : Loss 2917.5014\n",
      "Iteration 480 : Loss 2917.3676\n",
      "Iteration 481 : Loss 2917.2350\n",
      "Iteration 482 : Loss 2917.1036\n",
      "Iteration 483 : Loss 2916.9734\n",
      "Iteration 484 : Loss 2916.8444\n",
      "Iteration 485 : Loss 2916.7166\n",
      "Iteration 486 : Loss 2916.5899\n",
      "Iteration 487 : Loss 2916.4644\n",
      "Iteration 488 : Loss 2916.3401\n",
      "Iteration 489 : Loss 2916.2169\n",
      "Iteration 490 : Loss 2916.0949\n",
      "Iteration 491 : Loss 2915.9739\n",
      "Iteration 492 : Loss 2915.8541\n",
      "Iteration 493 : Loss 2915.7353\n",
      "Iteration 494 : Loss 2915.6177\n",
      "Iteration 495 : Loss 2915.5011\n",
      "Iteration 496 : Loss 2915.3856\n",
      "Iteration 497 : Loss 2915.2711\n",
      "Iteration 498 : Loss 2915.1577\n",
      "Iteration 499 : Loss 2915.0453\n",
      "Iteration 500 : Loss 2914.9340\n",
      "Iteration 501 : Loss 2914.8236\n",
      "Iteration 502 : Loss 2914.7143\n",
      "Iteration 503 : Loss 2914.6059\n",
      "Iteration 504 : Loss 2914.4986\n",
      "Iteration 505 : Loss 2914.3922\n",
      "Iteration 506 : Loss 2914.2868\n",
      "Iteration 507 : Loss 2914.1824\n",
      "Iteration 508 : Loss 2914.0789\n",
      "Iteration 509 : Loss 2913.9763\n",
      "Iteration 510 : Loss 2913.8747\n",
      "Iteration 511 : Loss 2913.7740\n",
      "Iteration 512 : Loss 2913.6742\n",
      "Iteration 513 : Loss 2913.5754\n",
      "Iteration 514 : Loss 2913.4774\n",
      "Iteration 515 : Loss 2913.3803\n",
      "Iteration 516 : Loss 2913.2841\n",
      "Iteration 517 : Loss 2913.1888\n",
      "Iteration 518 : Loss 2913.0943\n",
      "Iteration 519 : Loss 2913.0007\n",
      "Iteration 520 : Loss 2912.9079\n",
      "Iteration 521 : Loss 2912.8160\n",
      "Iteration 522 : Loss 2912.7249\n",
      "Iteration 523 : Loss 2912.6347\n",
      "Iteration 524 : Loss 2912.5452\n",
      "Iteration 525 : Loss 2912.4566\n",
      "Iteration 526 : Loss 2912.3687\n",
      "Iteration 527 : Loss 2912.2817\n",
      "Iteration 528 : Loss 2912.1954\n",
      "Iteration 529 : Loss 2912.1099\n",
      "Iteration 530 : Loss 2912.0252\n",
      "Iteration 531 : Loss 2911.9413\n",
      "Iteration 532 : Loss 2911.8581\n",
      "Iteration 533 : Loss 2911.7756\n",
      "Iteration 534 : Loss 2911.6939\n",
      "Iteration 535 : Loss 2911.6130\n",
      "Iteration 536 : Loss 2911.5327\n",
      "Iteration 537 : Loss 2911.4532\n",
      "Iteration 538 : Loss 2911.3744\n",
      "Iteration 539 : Loss 2911.2963\n",
      "Iteration 540 : Loss 2911.2189\n",
      "Iteration 541 : Loss 2911.1422\n",
      "Iteration 542 : Loss 2911.0662\n",
      "Iteration 543 : Loss 2910.9909\n",
      "Iteration 544 : Loss 2910.9163\n",
      "Iteration 545 : Loss 2910.8423\n",
      "Iteration 546 : Loss 2910.7690\n",
      "Iteration 547 : Loss 2910.6963\n",
      "Iteration 548 : Loss 2910.6243\n",
      "Iteration 549 : Loss 2910.5529\n",
      "Iteration 550 : Loss 2910.4822\n",
      "Iteration 551 : Loss 2910.4121\n",
      "Iteration 552 : Loss 2910.3427\n",
      "Iteration 553 : Loss 2910.2738\n",
      "Iteration 554 : Loss 2910.2056\n",
      "Iteration 555 : Loss 2910.1379\n",
      "Iteration 556 : Loss 2910.0709\n",
      "Iteration 557 : Loss 2910.0045\n",
      "Iteration 558 : Loss 2909.9387\n",
      "Iteration 559 : Loss 2909.8734\n",
      "Iteration 560 : Loss 2909.8088\n",
      "Iteration 561 : Loss 2909.7447\n",
      "Iteration 562 : Loss 2909.6811\n",
      "Iteration 563 : Loss 2909.6182\n",
      "Iteration 564 : Loss 2909.5558\n",
      "Iteration 565 : Loss 2909.4940\n",
      "Iteration 566 : Loss 2909.4327\n",
      "Iteration 567 : Loss 2909.3719\n",
      "Iteration 568 : Loss 2909.3117\n",
      "Iteration 569 : Loss 2909.2520\n",
      "Iteration 570 : Loss 2909.1929\n",
      "Iteration 571 : Loss 2909.1342\n",
      "Iteration 572 : Loss 2909.0761\n",
      "Iteration 573 : Loss 2909.0185\n",
      "Iteration 574 : Loss 2908.9614\n",
      "Iteration 575 : Loss 2908.9049\n",
      "Iteration 576 : Loss 2908.8488\n",
      "Iteration 577 : Loss 2908.7932\n",
      "Iteration 578 : Loss 2908.7381\n",
      "Iteration 579 : Loss 2908.6835\n",
      "Iteration 580 : Loss 2908.6294\n",
      "Iteration 581 : Loss 2908.5757\n",
      "Iteration 582 : Loss 2908.5225\n",
      "Iteration 583 : Loss 2908.4698\n",
      "Iteration 584 : Loss 2908.4176\n",
      "Iteration 585 : Loss 2908.3658\n",
      "Iteration 586 : Loss 2908.3144\n",
      "Iteration 587 : Loss 2908.2635\n",
      "Iteration 588 : Loss 2908.2131\n",
      "Iteration 589 : Loss 2908.1631\n",
      "Iteration 590 : Loss 2908.1135\n",
      "Iteration 591 : Loss 2908.0644\n",
      "Iteration 592 : Loss 2908.0157\n",
      "Iteration 593 : Loss 2907.9674\n",
      "Iteration 594 : Loss 2907.9196\n",
      "Iteration 595 : Loss 2907.8721\n",
      "Iteration 596 : Loss 2907.8251\n",
      "Iteration 597 : Loss 2907.7785\n",
      "Iteration 598 : Loss 2907.7323\n",
      "Iteration 599 : Loss 2907.6865\n",
      "Iteration 600 : Loss 2907.6411\n",
      "Iteration 601 : Loss 2907.5961\n",
      "Iteration 602 : Loss 2907.5515\n",
      "Iteration 603 : Loss 2907.5072\n",
      "Iteration 604 : Loss 2907.4634\n",
      "Iteration 605 : Loss 2907.4199\n",
      "Iteration 606 : Loss 2907.3768\n",
      "Iteration 607 : Loss 2907.3341\n",
      "Iteration 608 : Loss 2907.2918\n",
      "Iteration 609 : Loss 2907.2498\n",
      "Iteration 610 : Loss 2907.2082\n",
      "Iteration 611 : Loss 2907.1669\n",
      "Iteration 612 : Loss 2907.1260\n",
      "Iteration 613 : Loss 2907.0855\n",
      "Iteration 614 : Loss 2907.0453\n",
      "Iteration 615 : Loss 2907.0054\n",
      "Iteration 616 : Loss 2906.9659\n",
      "Iteration 617 : Loss 2906.9267\n",
      "Iteration 618 : Loss 2906.8879\n",
      "Iteration 619 : Loss 2906.8494\n",
      "Iteration 620 : Loss 2906.8112\n",
      "Iteration 621 : Loss 2906.7734\n",
      "Iteration 622 : Loss 2906.7359\n",
      "Iteration 623 : Loss 2906.6987\n",
      "Iteration 624 : Loss 2906.6618\n",
      "Iteration 625 : Loss 2906.6252\n",
      "Iteration 626 : Loss 2906.5890\n",
      "Iteration 627 : Loss 2906.5530\n",
      "Iteration 628 : Loss 2906.5174\n",
      "Iteration 629 : Loss 2906.4820\n",
      "Iteration 630 : Loss 2906.4470\n",
      "Iteration 631 : Loss 2906.4123\n",
      "Iteration 632 : Loss 2906.3778\n",
      "Iteration 633 : Loss 2906.3437\n",
      "Iteration 634 : Loss 2906.3098\n",
      "Iteration 635 : Loss 2906.2763\n",
      "Iteration 636 : Loss 2906.2430\n",
      "Iteration 637 : Loss 2906.2100\n",
      "Iteration 638 : Loss 2906.1773\n",
      "Iteration 639 : Loss 2906.1448\n",
      "Iteration 640 : Loss 2906.1127\n",
      "Iteration 641 : Loss 2906.0808\n",
      "Iteration 642 : Loss 2906.0491\n",
      "Iteration 643 : Loss 2906.0178\n",
      "Iteration 644 : Loss 2905.9867\n",
      "Iteration 645 : Loss 2905.9558\n",
      "Iteration 646 : Loss 2905.9252\n",
      "Iteration 647 : Loss 2905.8949\n",
      "Iteration 648 : Loss 2905.8649\n",
      "Iteration 649 : Loss 2905.8350\n",
      "Iteration 650 : Loss 2905.8055\n",
      "Iteration 651 : Loss 2905.7762\n",
      "Iteration 652 : Loss 2905.7471\n",
      "Iteration 653 : Loss 2905.7182\n",
      "Iteration 654 : Loss 2905.6897\n",
      "Iteration 655 : Loss 2905.6613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 656 : Loss 2905.6332\n",
      "Iteration 657 : Loss 2905.6053\n",
      "Iteration 658 : Loss 2905.5777\n",
      "Iteration 659 : Loss 2905.5502\n",
      "Iteration 660 : Loss 2905.5230\n",
      "Iteration 661 : Loss 2905.4961\n",
      "Iteration 662 : Loss 2905.4693\n",
      "Iteration 663 : Loss 2905.4428\n",
      "Iteration 664 : Loss 2905.4165\n",
      "Iteration 665 : Loss 2905.3904\n",
      "Iteration 666 : Loss 2905.3645\n",
      "Iteration 667 : Loss 2905.3389\n",
      "Iteration 668 : Loss 2905.3134\n",
      "Iteration 669 : Loss 2905.2882\n",
      "Iteration 670 : Loss 2905.2632\n",
      "Iteration 671 : Loss 2905.2383\n",
      "Iteration 672 : Loss 2905.2137\n",
      "Iteration 673 : Loss 2905.1893\n",
      "Iteration 674 : Loss 2905.1651\n",
      "Iteration 675 : Loss 2905.1411\n",
      "Iteration 676 : Loss 2905.1172\n",
      "Iteration 677 : Loss 2905.0936\n",
      "Iteration 678 : Loss 2905.0702\n",
      "Iteration 679 : Loss 2905.0469\n",
      "Iteration 680 : Loss 2905.0238\n",
      "Iteration 681 : Loss 2905.0010\n",
      "Iteration 682 : Loss 2904.9783\n",
      "Iteration 683 : Loss 2904.9558\n",
      "Iteration 684 : Loss 2904.9335\n",
      "Iteration 685 : Loss 2904.9113\n",
      "Iteration 686 : Loss 2904.8894\n",
      "Iteration 687 : Loss 2904.8676\n",
      "Iteration 688 : Loss 2904.8460\n",
      "Iteration 689 : Loss 2904.8245\n",
      "Iteration 690 : Loss 2904.8033\n",
      "Iteration 691 : Loss 2904.7822\n",
      "Iteration 692 : Loss 2904.7612\n",
      "Iteration 693 : Loss 2904.7405\n",
      "Iteration 694 : Loss 2904.7199\n",
      "Iteration 695 : Loss 2904.6995\n",
      "Iteration 696 : Loss 2904.6792\n",
      "Iteration 697 : Loss 2904.6591\n",
      "Iteration 698 : Loss 2904.6391\n",
      "Iteration 699 : Loss 2904.6194\n",
      "Iteration 700 : Loss 2904.5997\n",
      "Iteration 701 : Loss 2904.5803\n",
      "Iteration 702 : Loss 2904.5609\n",
      "Iteration 703 : Loss 2904.5418\n",
      "Iteration 704 : Loss 2904.5228\n",
      "Iteration 705 : Loss 2904.5039\n",
      "Iteration 706 : Loss 2904.4852\n",
      "Iteration 707 : Loss 2904.4666\n",
      "Iteration 708 : Loss 2904.4482\n",
      "Iteration 709 : Loss 2904.4299\n",
      "Iteration 710 : Loss 2904.4118\n",
      "Iteration 711 : Loss 2904.3938\n",
      "Iteration 712 : Loss 2904.3759\n",
      "Iteration 713 : Loss 2904.3582\n",
      "Iteration 714 : Loss 2904.3406\n",
      "Iteration 715 : Loss 2904.3232\n",
      "Iteration 716 : Loss 2904.3059\n",
      "Iteration 717 : Loss 2904.2887\n",
      "Iteration 718 : Loss 2904.2717\n",
      "Iteration 719 : Loss 2904.2548\n",
      "Iteration 720 : Loss 2904.2380\n",
      "Iteration 721 : Loss 2904.2214\n",
      "Iteration 722 : Loss 2904.2048\n",
      "Iteration 723 : Loss 2904.1885\n",
      "Iteration 724 : Loss 2904.1722\n",
      "Iteration 725 : Loss 2904.1561\n",
      "Iteration 726 : Loss 2904.1400\n",
      "Iteration 727 : Loss 2904.1241\n",
      "Iteration 728 : Loss 2904.1084\n",
      "Iteration 729 : Loss 2904.0927\n",
      "Iteration 730 : Loss 2904.0772\n",
      "Iteration 731 : Loss 2904.0618\n",
      "Iteration 732 : Loss 2904.0465\n",
      "Iteration 733 : Loss 2904.0313\n",
      "Iteration 734 : Loss 2904.0162\n",
      "Iteration 735 : Loss 2904.0013\n",
      "Iteration 736 : Loss 2903.9864\n",
      "Iteration 737 : Loss 2903.9717\n",
      "Iteration 738 : Loss 2903.9571\n",
      "Iteration 739 : Loss 2903.9426\n",
      "Iteration 740 : Loss 2903.9282\n",
      "Iteration 741 : Loss 2903.9139\n",
      "Iteration 742 : Loss 2903.8997\n",
      "Iteration 743 : Loss 2903.8856\n",
      "Iteration 744 : Loss 2903.8716\n",
      "Iteration 745 : Loss 2903.8577\n",
      "Iteration 746 : Loss 2903.8440\n",
      "Iteration 747 : Loss 2903.8303\n",
      "Iteration 748 : Loss 2903.8167\n",
      "Iteration 749 : Loss 2903.8033\n",
      "Iteration 750 : Loss 2903.7899\n",
      "Iteration 751 : Loss 2903.7766\n",
      "Iteration 752 : Loss 2903.7634\n",
      "Iteration 753 : Loss 2903.7504\n",
      "Iteration 754 : Loss 2903.7374\n",
      "Iteration 755 : Loss 2903.7245\n",
      "Iteration 756 : Loss 2903.7117\n",
      "Iteration 757 : Loss 2903.6990\n",
      "Iteration 758 : Loss 2903.6864\n",
      "Iteration 759 : Loss 2903.6739\n",
      "Iteration 760 : Loss 2903.6614\n",
      "Iteration 761 : Loss 2903.6491\n",
      "Iteration 762 : Loss 2903.6368\n",
      "Iteration 763 : Loss 2903.6247\n",
      "Iteration 764 : Loss 2903.6126\n",
      "Iteration 765 : Loss 2903.6006\n",
      "Iteration 766 : Loss 2903.5887\n",
      "Iteration 767 : Loss 2903.5769\n",
      "Iteration 768 : Loss 2903.5652\n",
      "Iteration 769 : Loss 2903.5535\n",
      "Iteration 770 : Loss 2903.5419\n",
      "Iteration 771 : Loss 2903.5305\n",
      "Iteration 772 : Loss 2903.5190\n",
      "Iteration 773 : Loss 2903.5077\n",
      "Iteration 774 : Loss 2903.4965\n",
      "Iteration 775 : Loss 2903.4853\n",
      "Iteration 776 : Loss 2903.4742\n",
      "Iteration 777 : Loss 2903.4632\n",
      "Iteration 778 : Loss 2903.4523\n",
      "Iteration 779 : Loss 2903.4414\n",
      "Iteration 780 : Loss 2903.4306\n",
      "Iteration 781 : Loss 2903.4199\n",
      "Iteration 782 : Loss 2903.4093\n",
      "Iteration 783 : Loss 2903.3987\n",
      "Iteration 784 : Loss 2903.3882\n",
      "Iteration 785 : Loss 2903.3778\n",
      "Iteration 786 : Loss 2903.3674\n",
      "Iteration 787 : Loss 2903.3572\n",
      "Iteration 788 : Loss 2903.3470\n",
      "Iteration 789 : Loss 2903.3368\n",
      "Iteration 790 : Loss 2903.3268\n",
      "Iteration 791 : Loss 2903.3167\n",
      "Iteration 792 : Loss 2903.3068\n",
      "Iteration 793 : Loss 2903.2969\n",
      "Iteration 794 : Loss 2903.2871\n",
      "Iteration 795 : Loss 2903.2774\n",
      "Iteration 796 : Loss 2903.2677\n",
      "Iteration 797 : Loss 2903.2581\n",
      "Iteration 798 : Loss 2903.2486\n",
      "Iteration 799 : Loss 2903.2391\n",
      "Iteration 800 : Loss 2903.2297\n",
      "Iteration 801 : Loss 2903.2203\n",
      "Iteration 802 : Loss 2903.2110\n",
      "Iteration 803 : Loss 2903.2018\n",
      "Iteration 804 : Loss 2903.1926\n",
      "Iteration 805 : Loss 2903.1835\n",
      "Iteration 806 : Loss 2903.1745\n",
      "Iteration 807 : Loss 2903.1655\n",
      "Iteration 808 : Loss 2903.1565\n",
      "Iteration 809 : Loss 2903.1477\n",
      "Iteration 810 : Loss 2903.1388\n",
      "Iteration 811 : Loss 2903.1301\n",
      "Iteration 812 : Loss 2903.1214\n",
      "Iteration 813 : Loss 2903.1127\n",
      "Iteration 814 : Loss 2903.1041\n",
      "Iteration 815 : Loss 2903.0956\n",
      "Iteration 816 : Loss 2903.0871\n",
      "Iteration 817 : Loss 2903.0787\n",
      "Iteration 818 : Loss 2903.0703\n",
      "Iteration 819 : Loss 2903.0619\n",
      "Iteration 820 : Loss 2903.0537\n",
      "Iteration 821 : Loss 2903.0454\n",
      "Iteration 822 : Loss 2903.0373\n",
      "Iteration 823 : Loss 2903.0291\n",
      "Iteration 824 : Loss 2903.0211\n",
      "Iteration 825 : Loss 2903.0131\n",
      "Iteration 826 : Loss 2903.0051\n",
      "Iteration 827 : Loss 2902.9972\n",
      "Iteration 828 : Loss 2902.9893\n",
      "Iteration 829 : Loss 2902.9815\n",
      "Iteration 830 : Loss 2902.9737\n",
      "Iteration 831 : Loss 2902.9659\n",
      "Iteration 832 : Loss 2902.9583\n",
      "Iteration 833 : Loss 2902.9506\n",
      "Iteration 834 : Loss 2902.9430\n",
      "Iteration 835 : Loss 2902.9355\n",
      "Iteration 836 : Loss 2902.9280\n",
      "Iteration 837 : Loss 2902.9205\n",
      "Iteration 838 : Loss 2902.9131\n",
      "Iteration 839 : Loss 2902.9057\n",
      "Iteration 840 : Loss 2902.8984\n",
      "Iteration 841 : Loss 2902.8911\n",
      "Iteration 842 : Loss 2902.8839\n",
      "Iteration 843 : Loss 2902.8767\n",
      "Iteration 844 : Loss 2902.8695\n",
      "Iteration 845 : Loss 2902.8624\n",
      "Iteration 846 : Loss 2902.8554\n",
      "Iteration 847 : Loss 2902.8483\n",
      "Iteration 848 : Loss 2902.8413\n",
      "Iteration 849 : Loss 2902.8344\n",
      "Iteration 850 : Loss 2902.8275\n",
      "Iteration 851 : Loss 2902.8206\n",
      "Iteration 852 : Loss 2902.8138\n",
      "Iteration 853 : Loss 2902.8070\n",
      "Iteration 854 : Loss 2902.8002\n",
      "Iteration 855 : Loss 2902.7935\n",
      "Iteration 856 : Loss 2902.7868\n",
      "Iteration 857 : Loss 2902.7802\n",
      "Iteration 858 : Loss 2902.7736\n",
      "Iteration 859 : Loss 2902.7670\n",
      "Iteration 860 : Loss 2902.7605\n",
      "Iteration 861 : Loss 2902.7540\n",
      "Iteration 862 : Loss 2902.7475\n",
      "Iteration 863 : Loss 2902.7411\n",
      "Iteration 864 : Loss 2902.7347\n",
      "Iteration 865 : Loss 2902.7283\n",
      "Iteration 866 : Loss 2902.7220\n",
      "Iteration 867 : Loss 2902.7157\n",
      "Iteration 868 : Loss 2902.7095\n",
      "Iteration 869 : Loss 2902.7033\n",
      "Iteration 870 : Loss 2902.6971\n",
      "Iteration 871 : Loss 2902.6909\n",
      "Iteration 872 : Loss 2902.6848\n",
      "Iteration 873 : Loss 2902.6787\n",
      "Iteration 874 : Loss 2902.6727\n",
      "Iteration 875 : Loss 2902.6666\n",
      "Iteration 876 : Loss 2902.6606\n",
      "Iteration 877 : Loss 2902.6547\n",
      "Iteration 878 : Loss 2902.6488\n",
      "Iteration 879 : Loss 2902.6429\n",
      "Iteration 880 : Loss 2902.6370\n",
      "Iteration 881 : Loss 2902.6311\n",
      "Iteration 882 : Loss 2902.6253\n",
      "Iteration 883 : Loss 2902.6196\n",
      "Iteration 884 : Loss 2902.6138\n",
      "Iteration 885 : Loss 2902.6081\n",
      "Iteration 886 : Loss 2902.6024\n",
      "Iteration 887 : Loss 2902.5967\n",
      "Iteration 888 : Loss 2902.5911\n",
      "Iteration 889 : Loss 2902.5855\n",
      "Iteration 890 : Loss 2902.5799\n",
      "Iteration 891 : Loss 2902.5743\n",
      "Iteration 892 : Loss 2902.5688\n",
      "Iteration 893 : Loss 2902.5633\n",
      "Iteration 894 : Loss 2902.5579\n",
      "Iteration 895 : Loss 2902.5524\n",
      "Iteration 896 : Loss 2902.5470\n",
      "Iteration 897 : Loss 2902.5416\n",
      "Iteration 898 : Loss 2902.5362\n",
      "Iteration 899 : Loss 2902.5309\n",
      "Iteration 900 : Loss 2902.5256\n",
      "Iteration 901 : Loss 2902.5203\n",
      "Iteration 902 : Loss 2902.5150\n",
      "Iteration 903 : Loss 2902.5098\n",
      "Iteration 904 : Loss 2902.5046\n",
      "Iteration 905 : Loss 2902.4994\n",
      "Iteration 906 : Loss 2902.4942\n",
      "Iteration 907 : Loss 2902.4891\n",
      "Iteration 908 : Loss 2902.4839\n",
      "Iteration 909 : Loss 2902.4788\n",
      "Iteration 910 : Loss 2902.4738\n",
      "Iteration 911 : Loss 2902.4687\n",
      "Iteration 912 : Loss 2902.4637\n",
      "Iteration 913 : Loss 2902.4587\n",
      "Iteration 914 : Loss 2902.4537\n",
      "Iteration 915 : Loss 2902.4487\n",
      "Iteration 916 : Loss 2902.4438\n",
      "Iteration 917 : Loss 2902.4389\n",
      "Iteration 918 : Loss 2902.4340\n",
      "Iteration 919 : Loss 2902.4291\n",
      "Iteration 920 : Loss 2902.4242\n",
      "Iteration 921 : Loss 2902.4194\n",
      "Iteration 922 : Loss 2902.4146\n",
      "Iteration 923 : Loss 2902.4098\n",
      "Iteration 924 : Loss 2902.4050\n",
      "Iteration 925 : Loss 2902.4003\n",
      "Iteration 926 : Loss 2902.3956\n",
      "Iteration 927 : Loss 2902.3909\n",
      "Iteration 928 : Loss 2902.3862\n",
      "Iteration 929 : Loss 2902.3815\n",
      "Iteration 930 : Loss 2902.3768\n",
      "Iteration 931 : Loss 2902.3722\n",
      "Iteration 932 : Loss 2902.3676\n",
      "Iteration 933 : Loss 2902.3630\n",
      "Iteration 934 : Loss 2902.3584\n",
      "Iteration 935 : Loss 2902.3539\n",
      "Iteration 936 : Loss 2902.3493\n",
      "Iteration 937 : Loss 2902.3448\n",
      "Iteration 938 : Loss 2902.3403\n",
      "Iteration 939 : Loss 2902.3358\n",
      "Iteration 940 : Loss 2902.3314\n",
      "Iteration 941 : Loss 2902.3269\n",
      "Iteration 942 : Loss 2902.3225\n",
      "Iteration 943 : Loss 2902.3181\n",
      "Iteration 944 : Loss 2902.3137\n",
      "Iteration 945 : Loss 2902.3093\n",
      "Iteration 946 : Loss 2902.3049\n",
      "Iteration 947 : Loss 2902.3006\n",
      "Iteration 948 : Loss 2902.2963\n",
      "Iteration 949 : Loss 2902.2919\n",
      "Iteration 950 : Loss 2902.2877\n",
      "Iteration 951 : Loss 2902.2834\n",
      "Iteration 952 : Loss 2902.2791\n",
      "Iteration 953 : Loss 2902.2749\n",
      "Iteration 954 : Loss 2902.2706\n",
      "Iteration 955 : Loss 2902.2664\n",
      "Iteration 956 : Loss 2902.2622\n",
      "Iteration 957 : Loss 2902.2580\n",
      "Iteration 958 : Loss 2902.2538\n",
      "Iteration 959 : Loss 2902.2497\n",
      "Iteration 960 : Loss 2902.2456\n",
      "Iteration 961 : Loss 2902.2414\n",
      "Iteration 962 : Loss 2902.2373\n",
      "Iteration 963 : Loss 2902.2332\n",
      "Iteration 964 : Loss 2902.2291\n",
      "Iteration 965 : Loss 2902.2251\n",
      "Iteration 966 : Loss 2902.2210\n",
      "Iteration 967 : Loss 2902.2170\n",
      "Iteration 968 : Loss 2902.2129\n",
      "Iteration 969 : Loss 2902.2089\n",
      "Iteration 970 : Loss 2902.2049\n",
      "Iteration 971 : Loss 2902.2009\n",
      "Iteration 972 : Loss 2902.1970\n",
      "Iteration 973 : Loss 2902.1930\n",
      "Iteration 974 : Loss 2902.1891\n",
      "Iteration 975 : Loss 2902.1851\n",
      "Iteration 976 : Loss 2902.1812\n",
      "Iteration 977 : Loss 2902.1773\n",
      "Iteration 978 : Loss 2902.1734\n",
      "Iteration 979 : Loss 2902.1695\n",
      "Iteration 980 : Loss 2902.1657\n",
      "Iteration 981 : Loss 2902.1618\n",
      "Iteration 982 : Loss 2902.1580\n",
      "Iteration 983 : Loss 2902.1541\n",
      "Iteration 984 : Loss 2902.1503\n",
      "Iteration 985 : Loss 2902.1465\n",
      "Iteration 986 : Loss 2902.1427\n",
      "Iteration 987 : Loss 2902.1389\n",
      "Iteration 988 : Loss 2902.1351\n",
      "Iteration 989 : Loss 2902.1314\n",
      "Iteration 990 : Loss 2902.1276\n",
      "Iteration 991 : Loss 2902.1239\n",
      "Iteration 992 : Loss 2902.1202\n",
      "Iteration 993 : Loss 2902.1164\n",
      "Iteration 994 : Loss 2902.1127\n",
      "Iteration 995 : Loss 2902.1090\n",
      "Iteration 996 : Loss 2902.1054\n",
      "Iteration 997 : Loss 2902.1017\n",
      "Iteration 998 : Loss 2902.0980\n",
      "Iteration 999 : Loss 2902.0944\n",
      "Iteration 1000 : Loss 2902.0907\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= L_rate * dW\n",
    "    b -= L_rate * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 1 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e1c034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2901.1467481652817"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04db624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAArD0lEQVR4nO2de5QV1Z3vP79+YaMZWgRFGgwkMWZFoxJbc2/wzkpklKhRyWiIyUyimTi4boyOmSwITnIVWZmxhZk4OndMZDA3evNQogmgJGOMxGQgY6R5hGgyXImPQAMCKvigobvhd/841XAeVd11ut5Vv89avfqcfarO2aeqzrf2/r22qCqGYRhGvmhIugOGYRhG+Ji4G4Zh5BATd8MwjBxi4m4YhpFDTNwNwzBySFPSHQAYM2aMTpo0KeluGIZhZIq1a9fuVtWxbq+lQtwnTZpEV1dX0t0wDMPIFCLyktdrZpYxDMPIISbuhmEYOcTE3TAMI4eYuBuGYeQQE3fDMIwckopomaKzdH03Cx/bxLY9PYxva2X29FOYMaU96W4ZhpFhTNwTZun6bm764W/p6TsIQPeeHm764W8BTOANwxg2ZpZJmIWPbTos7AP09B1k4WObEuqRYRh5wMQ9Ybbt6amr3TAMww8m7gkzvq21rnbDMAw/mLgnzOzpp9Da3FjR1trcyOzppyTUI8Mw8oA5VBNmwGlq0TKGYYTJkOIuIkcBvwRGONs/pKq3iMhk4AHgOGAt8GlV7RWREcD9wFnAK8AnVPXFiPqfC2ZMaTcxNwwjVPyYZQ4A56nqGcCZwEdE5L8BtwN3qOq7gNeAzznbfw54zWm/w9nOMAzDiJEhxV1LvOk8bXb+FDgPeMhpvw+Y4Ty+zHmO8/o0EZGwOmwYhmEMjS+bu4g0UjK9vAv4V+APwB5V7Xc22QoM2BXagS0AqtovInspmW52V73nLGAWwEknnRTsWxhGRrHsZCMqfIm7qh4EzhSRNuBHwHuCfrCqLgIWAXR0dGjQ9zOMrJFkdrLdVOpk4xJ4Yj7s3QqjJsC0m+H0mUn3alDqCoVU1T3Az4H/DrSJyMDNYQLQ7TzuBiYCOK+PouRYNQyjjKSykwduKt17elCO3FSWru8ect9CsnEJPHID7N0CaOn/IzeU2lPMkOIuImOdETsi0gqcD/yekshf4Wx2FbDMebzceY7z+kpVtZG5YVSRVHaylbyokyfmQ1/VOenrKbWnGD9mmROB+xy7ewOwRFUfFZHfAQ+IyNeA9cC9zvb3Av9XRDYDrwJXRtBvw8g849ta6XYR8qizk63kRZ3s3Vpfe0oYUtxVdSMwxaX9eeAcl/b9wMdD6Z1h5JjZ00+psLlDPNnJSd1UMsuoCY5JxqU9xVj5AcNIiBlT2rntz99He1srArS3tXLbn78vcsemlbyok2k3Q3PVja+5tdSeYqz8gGEkSBLZyVbyok4GomIyFi0jafB1dnR0aFdXV9LdMAzDyBQislZVO9xeM7OMYRhGDjFxNwzDyCFmczcMIzIsEzY5TNwNw4gEW/w9WcwsYxhGJFgmbLKYuBuGEQmWCZssJu6GYUSCLf6eLCbuhmEMytL13UztXMnkuSuY2rnSd/VIy4RNFnOoGobhSRCnqGXCJouJu5EaLGwufQzmFPVzbmzx9+QwcTdSgYXNpRNzimYXs7kbqcDC5tKJOUWzi4m7kQpshJhOzCmaXUzcjVRgI8R0klTNeSM4ZnM3UkFSqxIZQ2NO0Wxi4m6kAgubq5+sRhdltd9Zw8TdSA02QvRPVqOLstrvLGI2d8PIIFmNLspqv7OIibthZJCsRhdltd9ZxMwyGcfsl4OwcUnmFjX2y/i2VrpdBDHt0UVZ7XcWsZF7hhmwX3bv6UE5Yr/0W9gp12xcAo/cAHu3AFr6/8gNpfYckNX486z2O4uYuGcYs18OwhPzoa9qhNjXU2rPAVmNP89qv7OImWUyjNkvB2Hv1vraM0hWo4uy2u+sYeKeYcx+OQijJjgmGZf2AJiPw8gKZpbJMGa/HIRpN0Nz1U2uubXUPkzMx2FkiSHFXUQmisjPReR3IvKsiPyN0z5PRLpFZIPzd1HZPjeJyGYR2SQi06P8AkXG7JeDcPpMuOQuGDURkNL/S+4KFC1jPg4jS/gxy/QDX1LVdSLyNmCtiDzuvHaHqv5j+cYi8l7gSuBUYDzwMxF5t6pW/iqMUDD75SCcPjPU0EfzcSSPmcX8M+TIXVW3q+o65/EbwO+BwY7mZcADqnpAVV8ANgPnhNFZw0gSq1yZLGYWq4+6bO4iMgmYAvzaafqCiGwUkW+JyLFOWztQ7snaisvNQERmiUiXiHTt2rWr/p4bRsyYjyMkNi6BO06DeW2l/z5zD8wsVh++xV1EjgEeBm5U1deBbwDvBM4EtgP/VM8Hq+oiVe1Q1Y6xY8fWs6thJIL5OEIgQHKZmcXqw1copIg0UxL276rqDwFU9eWy1/8NeNR52g1MLNt9gtNmGLEShX3WfBwBGSy5bAj/iIX+1oefaBkB7gV+r6pfL2s/sWyzjwHPOI+XA1eKyAgRmQycDDwdXpcNY2jMPptSAiSXmVmsPvyM3KcCnwZ+KyIbnLa/Az4pImcCCrwIXAugqs+KyBLgd5Qiba6zSBkjbgazz9rIO0ECJJfZgi71MaS4q+oqQFxe+vEg+/w98PcB+mUYgTD7bEqZdnPJxl5umqkjuczMYv6x8gNGLjH7bP3EEkM+YFfPaSnmNGHibuQSW3C7PmJd/i7k5DLDHastY+QSC1usD4shzx82co8QS5VOliTts1k79+ajyB8m7hFhq7wXlyye+6h8FFm7yeUJM8tEhE1zi0sWz30UMeRL13ez6kd38+C+v+YPIz7Fg/v+mlU/ujuTuQZL13cztXMlk+euYGrnykx8BxP3iLBpbnHJ4rmPwkexYcUi5ssiJjTspkFgQsNu5ssiNqxYFF7HYyCrCXFmlokIC8UrLlk992H7KK7p/Q4jG3or2kZKL9f0fge4NbTPiZqsJsTZyD0iLFW6uNi5LzG+4ZW62tNKFmdiYOIeGRaKV1zs3JfY3zqurva0ktU6/qKqSfeBjo4O7erqSrobhmGEycYl9C+7nqaD+w839TceRdNl/5KpJKbq6CcozcTScMMWkbWq2uH2mtncDQML2YuE02eWBKas1EBTBksNZLVgmY3cjRqKJnRpHpkZxmDYyN3wTRYTcIKS1WiIQGxcUlO8a+nBqYW6qecdc6gaFWQxAScoWY2GGDYuS931L7ueVT+6O3Ox3IY3Ju5GBYUTOrIbDTFsXJa6azq4nxt5oKIt7zf1vGPiblRQOKGjgHHpHkvajZfa+PNte3pKI/07ToN5baX/PhazNpLHxN2owEvoPvyesZmrreGXwsWleyxpt02Pq2m76pina0w4PHKDCXwGMIeqUYFb2NeH3zOWh9d259rJWqjl21yWuutvPIp/PnRlxWatzY3MaX4QeqpMcn09JdNOxkIai4aJu1FDtdBN7VxZvGiSPOOy1F3TtJs59+BU/rMqWmbksh3u7+Fh2jHSg4m7MSRFdLLmCpewR7el7mbgMhN7coJjkqnCw7QTOV7fxajBbO5pIG0Oq6r+XHXM066b5dnJmhtcwh7rsplPuxmaq85zc2upPW6CfpeCYeKeNGm7YF3681X9Jle0/Kpis1xHk+QJl7DHwzZzP5w+Ey65C0ZNBKT0/5K7khktB/0uBcPMMkkz2AWbkh9Q08H9zD/6Yf5z5DTLXswaXrbxemzmLiacRAjjuxQIE/ekSdsF6/G5I3t2sHreeTF3xgjMqJTZzIOQp+8SA2aWSRqPC3Nf67hk4sq9fiitx6bLL5AXova3pMlm7jDs9UhT+F3SjIl70rhcsP2NR3HzW5cnU+fD7QfU2AIH3kiPXyAL+BHtOPwtabKZE3A90pR9l7QzZMlfEZkI3A+cACiwSFXvFJHRwIPAJOBFYKaqviYiAtwJXATsA65W1XWDfUYSJX9TVda2Krxr3luX8+03z6nZrL2tldVzYzCNVIeb9b4FPa/WbjdqInzxmej7kzUGRLvcd9HcWitEd5zmYWbI73Gd2rnSdX3Z2K7tnBG05G8/8CVVXScibwPWisjjwNXAE6raKSJzgbnAl4ELgZOdvw8A33D+p4bUlbWtcljdN3eF62axxZVXO9DmtblvZ44sd/w6ydPmb4kBy5mIjyHNMqq6fWDkrapvAL8H2oHLgPucze6jlAOB036/lngKaBORE8PueBDSXtY2dcW7vOzw5shyx69oF/C4pu7azjF12dxFZBIwBfg1cIKqbnde2kHJbAMl4S+fa2512qrfa5aIdIlI165du+rtdyDSPnpIXZXCoI6stCVpRY1f0S6ggzB113aO8S3uInIM8DBwo6q+Xv6algz3da3Xp6qLVLVDVTvGjh1bz66BiW30MExRS12VwiCOrLQlaXkR5g3Ir2gX0EGYums7x/haQ1VEmoFHgcdU9etO2ybgQ6q63TG7PKmqp4jIPc7j71dv5/X+cTtUY1kz069TLe94OQ1bR0PL0emoERLFuYqjBorVWSk8gzlU/UTLCCWb+quqemNZ+0LglTKH6mhVnSMiFwNfoBQt8wHgLlWtDf0oI5fRMgWMhHBlXhu+JnVJ3viyeK5s8GAQPFpmKvBp4LcissFp+zugE1giIp8DXgIGrqgfUxL2zZRCIT87/K5HR+T1uxOMhEhVmKdXVmE1SZZcyGLUStrKVhipY0hxV9VVgHi8PM1lewWuC9iv7JNQqnTqwjxdFobwJCkxzWJae1ZuSGY6SgyrLRMVbqIWQSRE9Sh9X29/uhbWcFkYwjMpaqDEQdxCENO5CpUs3JCqTUcDznQwgY8BKz8QFTFEQrilcr+2r89120TDPE+fWbJdz9tT+n/h7fQ3HlWxyUFphp49lVE1Sz8fT1RNFqNWshBGaSV6E8VG7lEScqnU6lH6WwdqR+lepClJZOnBqazqu4YbeYDx8grb9Dja5A2Okaob06E++MmX4xHZtJS19YvbjChtJo+smI5yiol7RnCzpfslbUkiCx/bRHfvB3mIDx5ue2HEp9w3djPfGCXSfkPKgunIjZz4CcwsEzfDTJZxK5ngRVtrc6qTRNKSCWxETBZMR9VkJenOBzZyj5MADia/gtja3Mi8S091FfO0hEiOb2utmXm8xjGM5s3ajVtHx9SrlPPo38Lab4MeBGmEs66Gj3496V4NThZMR9V4+An2/eRmzv/xmMR/O/VgI/c4CeBg8rKZHzvS3yg9UB3tkHGrL/IPenXJqVpOYwtceHuMPUspj/4tdN1bEnYo/e+6t9TuhyRr+1Q709Ms7ODpDzhq345U/HbqwcQ9TgI4mLwKLt1yyamsnnseL3RezOq553mOJtJUCdOtvsi5H/s8jR+7uzJi5bJ/Tb8YxMHab9fXXk6OzAyx4OEP2KbHVTxPUxVZL8wsEycBHEwDoj1cs0raKmG6Zwin3EGYFOrha/FqL8drtviTL2fLXBIXLjkP+7SFBf21xybtviMT9zgJmCwTpGSCm517oN1IOdLoLuTSWNtWjdessOfVI5FIllx0BBc/wYK3Lmf5gdryWGn/7ZhZJk4STJaZPf0Urmj5FatabuD5EZ9iVcsNXNHyq1SFSBoenHV1fe3l+A07tOSiI1T5Cc68eFYma9DbyD1uEopNntG4mo82L6bp4H4AJshuOhsX09R4BkdqvhmpZCAqZjjRMlmo7UN6IrncmDGlnfYtjzJx3UKO113slLFsef9szp7ykaS7Nii+6rlHTRIlfwtHFsvaGuGQ8gXPY1lfIQgpLq8ctOSvkQN071bX0p5e7WGS5lFZIaieLXqJVULJRYNFcqXiOsloeWUT94LwMmMYR+1ataX26PAqQdz10qv8/L92meAnQcqSi9IWyVVDRDVyoh70mLgXhJ/2n8GnG3+GlA3TVeGnB8/gMxF+rteo7LtP/fHw+kyJ15yvh5zUHUlTXZrUR3JFUCMnjnUXLFomSwTINLyg6TcVwg4gUmqPEq/RV7WnJwtJIblKCAqatRpi1qtXgl5qolEiqJETR1KhjdyzQsCFD05gd13tYeE1Kru0YRVzmpYwXnazTcewoH8mj+w5N9K+BCZB22uoU/igi2iEvAhH0AS9yInAjBWHKcrEPSsEzDQUj6mlRFx+dfb0U2oiIS5rWMVtzYsZKb2AE5bZvJjRzS3AxZH2JxAJ1ScPfQof9CYVwU0u8jWNgxKyGSsOU5SZZbLCYJmGfswECZVfdasjM+/ohw8L+wAjpZc5zQ9G2pfAeN0II75Bhj6F91qw3M9C5mCLcIRAHKYoG7lnBS+nTjVeI6gEIyRqRmXzdrpuN7JnR+R9CURCa62GPoUPUs4AsrsIR4qIwxRVWHHPXOx1GJmGaYmQyKo4JHSDDH0KP1ghMj8LlGdxQfEUErUpqpBmmTTVNveNW10ar4Us0i6SWVyhJ0FCn8KPmujxgvgz8WVxQfECUsjyA1M7V7qOhNrbWlk997zY+hGYjUtg2XVwsMx+3diSjTroWYwXTzANPexomf5l1x+uMwSl0FTXTGUrT5FqrPxAFanPiKuH6ptzCm7WvkiLiageEqyNHuYUfunBqazqu4YbeYDx8grb9DjaZbe7upuTNLMUUtxTnxHnlyfmw6G+yrZDfamveZEVqkfLq/Z71OHJWG30hY9torv3gzzEBw+3rWq5gQnikvOQdhOf4Ukhbe6pz4jzi4WkRYabX6Z6qTVPUlYbfen6bqZ2rmTy3BWeJskF/TPZpy2VjeYHyTRDiruIfEtEdorIM2Vt80SkW0Q2OH8Xlb12k4hsFpFNIjI9qo4HwS32OjXlReshobjrIuAWW35730x6GOHvDVJyg3W7SbnNPpYfOpcFzZ83J2mO8GOW+Tbwv4H7q9rvUNV/LG8QkfcCVwKnAuOBn4nIu1X9LPYYL6nPiPODhaRFhpv/Zfmhc5FeuHPsIz5qo6fjBut2kxpwnpZ7Z1qbGznz4lkw5dY4u2dEyJAjd1X9JeBy9bpyGfCAqh5Q1ReAzUDt4oNGOFhIWmR4+V+6/uT8iiXYuPD2VId1Dla4LfMz16wTYvE1N4I4VL8gIp8BuoAvqeprQDvwVNk2W522GkRkFjAL4KSTTgrQjYKTxaiTDOBWE8fVL5Oy2ujVeAUPZC7sN2+EXHzNjeE6VL8BvBM4E9gO/FO9b6Cqi1S1Q1U7xo4dO8xuGFHf/YtKXX6ZqgWV0yLskKPggbwxWPG1kBjWyF1VXx54LCL/BjzqPO0GytPfJjhtRhTEcPcvMnnwy6SynG4WE9jCJoZIt2GJu4icqKrbnacfAwYiaZYD3xORr1NyqJ4MPB24l0XEzw8go2s7GvHi+yYVh+h6DUj++BQ899PiCH4M9ZWGFHcR+T7wIWCMiGwFbgE+JCJnUvLLvAhcC6Cqz4rIEuB3QD9wXVSRMpkr/FUPfkfknnf/Lf4KQBmZIfLrPa5ZoNeApOtbHI7fKcIMNIZIt0zWlqlevABKdsTcePzvOM3jrl5V5+P2ye5heNXEVP/EiIZYrne/11xQ5rVRu8iiB3mvaxPCTGmw2jKZzFCNY/3BRAnbHpeyjEmjPmK53uPKdq7H7JCSRLDIiNgRn0lxz1XhLzf8Zp72vOb/PfP+Q4mJ6lT+OMpEx3K9x5Xt7Fbu2b1iT2oSwbJKJsXdK8Ekc4W/vPBb77yeiz/nP5Q4RDepdQBiud7jqrHvlnjX8VepTgTLKpkU99zH7vrNPHX7QTY0l2q6l5PzH0pcopuUOTCW6z3ObOdqc8RHv26Z1hGQSYcq5Dxaph7cnDJQqDjiuBZfmTx3hasrUIAXOi8O7XPcsOvdcCOXi3XkIcEkFLzKD+RYzKsJxSbtI3IhyXUA7Ho36iWTZhnjCEk4+AYlgXIIgW3SAzHeQ6wfmntzoJErTNwzTJwOPl83EZ8iGTaBRddnnY/crANgFILMmmWMwR18YQpOdRLNwE0EqPychMohBK6fUkeMt5lHjKxg4p5h4or3X/jYJs4/+AvmtCxhvOxmm45hQf9MFj7WUil0CS77F0h0Y6jzYRhxY2aZDBNXvH/H64/T2byYCQ27aRCY0LCbzubFdLz+eOWGWV32L64Yb8OIERP3DBOXg++mlh8wUnor2kZKLze1/KByw4AimZhzOIoYb6uzbySMmWUyTFy1uk9gt7/2AKsS+bbr10M9hZnCXNHK6uwbKSC74m4F/4F4HHziYZMWN3OLX5GsOn8b3rqcnr7K5XYDOYeTFFirs2+kgGyaZRIKuSssYdukXc7fnL67ubRhVc2mw3YOx7CMmSdROJZDNvOkLj/CCJ1sinuSP9wiErZN2uX8jZRe5jTVCtawncP1CmyY4hm2YznkwUxSBdCMeMmmuCcYcldYwqw97XGexssrFc8DOYfrEdiwZ4Jhz3RCHszkfj0EA8iquCcdcmeREMHwOE/7R44LL/uzHoENeyYY9kwn5MFM7tdDMICsOlQ91h9c887rubFzZbSV8ywSIjge52/khfNZfXpIVRzridyJYiYYZvRNyElWSRZAM+Ijm+Lu8sNd887r+cyat9PjCEYooXRupC0SIotRQwFCJuv+HD/vGUGGaqglekNeTHn29FNc12S1Amj5IrP13KuJq6a39wK/UrJHx0n1LAJsMezh4HUcz/gUPPfTwDH7UBLP+89+ibP/8C/Du6GFfBMPevOx+vLpIJf13KuJzY6YpjokaZtFZBW3mcTJF8Bvvjcs85ubw/L8g7/gtHX3Agfqfr/D24R4ToPkR0SScGaETjYdqi7Etq5qmuqQWNRQeFRHAz3302E7Wd0GFHOaltA6IOx1vl/aiCLaZs3ye9gx710cumUUO+a9izXL7wnazcKTG3GPbSGFONeaHIqko4byTIAbp9uAYry4l3DQvVsyF3kV9ix5zfJ7OG3tVxnHLhoExrGL09Z+1QQ+ILkR91gXUggz5jsIaZpF5I0AN063gcY2HeO6rcLw4+sTCskNe5Y8cd1CWqsK07VKLxPXLRzW+xklciPuUBL41XPP44XOi1k997z82//SNIvIGwFunG4DjQX9MzmglYJ/SF1+gH5NNQmW4Ah7lny87vJod5/tGP7IjUO1sITsaDMcAoZrVjss533tF0ifVGwj1TsN4MdnkqAzPexqpDtlLOOoFfidMoZxgXpabIYUdxH5FvBRYKeqnua0jQYeBCYBLwIzVfU1ERHgTuAiYB9wtaqui6br2cRCyDJEiDfOv5Hv0yL9FW3ipe5+fCYJO9PDrEa65f2zGbX2qxWmmR5tYctZs03cA+DHLPNt4CNVbXOBJ1T1ZOAJ5znAhcDJzt8s4BvhdDM5wqyeZwWbisuovp2u7TUZE359Jjlypp996bU8c9bX2MFYDqmwg7E8c9bXOPvSa5PuWqYZcuSuqr8UkUlVzZcBH3Ie3wc8CXzZab9fS5lRT4lIm4icqKrbQ+txjIQdzxvXgtZG8lTP0B48dBwTGmptyK/pMYxuO7Z+00/IWatJc/al14Ij5uOcPyMYw7W5n1Am2DuAE5zH7UB5hs9Wp61G3EVkFqXRPSeddNIwuxEtYYtxFIlWsWUaZrHMQUK4DQoWNszktubFFcsV7tMW7mq+hnlfvLX+D4mrhIORWQI7VFVVRaTuGgaqughYBKXyA0H7EQVhi3HYBZuCzix87x9BsbTQfQ8puvm4DQqWHToX+mB20xLGyyts0+P4Z67k3ItnDf+DzJluDMJwxf3lAXOLiJwIDBgUu4GJZdtNcNoyQbXgtI1s5rV9fTXbDVeMwy7YFHRm4Xv/kCMzQk9fT7pSZ9WNpeP1S+jm3JrNlh06l66R55sz3YiF4Yr7cuAqoNP5v6ys/Qsi8gDwAWBvrPb2AKM3N8FpbhCaG4W+g0cmFkHEOOwQsqAzC7/7696trmF7Xu1D4XVT2bBiETOefLj+85dkjR2XG0tny71oLyw/VCnwoRexi4sUzYoM//gJhfw+JefpGBHZCtxCSdSXiMjngJeAgTP9Y0phkJsphUJ+NoI+uxNw9OYmOH2HlLbWZo4e0RTaaCvMELKgZh6/+7/MGNc45FJ7/bjdVC5tWMWcvsWw17FJ13P+kgwLdLmxtHKALzcvYfmBI+Ke2ZK6Sc+KjGHjJ1rmkx4vTXPZVoHrgnZqWAQcvXmNYvf29LHhlgvC6GHoBDXz+N3/tt6PuzoDb+v7OHcOo99uN5U5TUsq3h+Avh72/eRmzv/xmMFvriFU6hyuD8Br9jJeXqG9rTW1Jhjf3zemWZHlf4RPfjJUA47esrg6TVAzj9/9u/7kfOa+XhLgAWfggv6ZrP2T84fVb7ebSvX6qQMctW8H3QeGWIAlYFhgEB/AYLOatJpg6vq+McyKrIRwNORH3AOO3rK6Ok1QM4+f/UvHppflvZVmhttC9D3sl3GM7Kl1z2zT4yqeuzp8A4YFBnFMhz2rqYfhjnbr+r4xrF9g+R/RkB9xDzh6C9vZmSeiODY1N5WN82vO3z5tYUF/rUC7mtAChAUGcUyHPavxS5DRbl3fN4ZkqW17eko+l6YljJfdbNMxLOifySN7aiOODP/kR9xDSOoI09mZNyI/Ni7nb8Fbl7P8wDk1m4ZtKgtikps9/RRmP7S/YlbT3CgsjHjGF2S0W9f3jSFZ6qpjnmZO35HZzwTZTWfzYkY3twAXh/Y5RSM/4g6W1JF1qs7fmeu7aY3BVPbh94zlO0/90bXdF9UpeArtWx6FJ4e5XqoPgsw26jZBRvy7mtP8ICP7K53pI6WXOc0PAsPI3jWAvIl7UCyeNz58HOu4TGU//y/3euJe7eUsfGwTfYcq1f1C/iPYeqk+CDLbSJsJcmTPjrraDX+YuA9g8bzxUcexjsNUFmQUXPd6qSFdS0EDAFJlgkzTovM5IlcrMQVisHheI1xSdqyDLBtXz3qpYYYPxrqsZNTYcpGRUNyRe7VZwG3kALEtflAoEl5oopogo2C3fbczhnZcBD7kkajf0XfqE4SswmUkFFPc3cwCCC5LJ9jUMApSNg0PYoN223fbe+fQ/ttbUlFrPTMJQhYMETpSqhiQLB0dHdrV1RXfB95xmsdIvUrgm1ttwekoqL65Qv6OdUqc81M7V7o6XjNbxMyoQETWqmqH22vFHLl7Tv8VRk1M/AeZe4owDU/JSDSKBWKMbFBMcfc0C0yELz4Tf38KyNKDU1l44C627e9h/FGtzD54CqTdNhwBUdvDs1gzyQiHYop7ztafzBpuduDZD/0GlMMx47HbhhMwo8RhD89qzaQkSb0D2ifFFPeAZoG8nPy6CFH8XGvnH6z1/Xim04ctxAnlONRTQmC411zaEpbSTmYc0D4oprgHIE8n3zd1iJ8fEarH3luzbRRCnNBKTn7t4UGvuVQlLKWcPFWoLGYS04BA7N0C6BGB2LhkyF0HO/m5xWfS0YAIde/pQTkiQkvXVy6jW4+9t2bbKBKgEoq795s8VchrLiHy5IAuprjXIxAbl5RCJ+e1wR2n0fH6465vmcWT7xtP8dtScWw2rFjkS4RmTz+F1ubGirbmRqG5oXJNI1fbcBRC7BVfH3HcvdtxcPvOSQvO0vXdTO1cyeS5K5jaubLmZp0ngmQrp41iirtfgXAZ4Xe23MulDatqds3iyfeNp8hJxbGZ03e367GpFiG31PmFV5zBJ86ZSKOUBL5RhMvPcjEnRCHECaW/z5jSzq2Tn2X1iBt4fsSnWD3iBm6d/GzNd05ScPzOxvKC3xtuFiimuPsViEEWP65oy+jJ942b+Llk9I6UXuY01Zq23ERoxpR2Vs89jxc6Lz6cTLN/3QP8ovl6nh/xKX7RfD371z1QKyJRCPHpM0sJVKMmlr7XqImxJFStWX4Pl7zUSbvspkGgXXZzyUudrFl+T8V2SQpO0UxCearZU0yHqt9QSI8RfpoWP44lcsctusijFk/1Wqh+RWjDikXMl0UVCzbM10UsWNHEjCllNb2jSoBKIOlo4rqFtFYtCt4qvUxctxAuvfZwW5IRL0mbhJIgLw7oYoq7X4HwEDEZNYHVX0w+dTvWyJ0q8dt3+3tc1zzd23I87a313/iu6f0OIxtqF2y4oW8x3PFw7XlKQfZnUI7XXaUJUE17bdGxpATHkqCySzHFHfwJRMqTnZIM21rQ9wnm6N01C0PfqZ8cVs2S8Q2vuLYfK2/C3jdLT3JWY3+njGUctQuC7JQxjEugP25YElR2KabN3S8J2WL9kuSU+b43z2Fu3zVsPTSGQypsPTSGuX3XcN+btWue+mF/q7uc1Qxsc1Rjf8v7Z9OjLRVtPdrClvfPTqhHteTJBl00ijty90uKTQBJTpnHt7WyfM+5FQtDQ+nH74uqLNORp15E//rv0nRw/+FNFFerRW5q7J996bWsoWR7P153s1PGsOWs2ZxdZm9PA3mxQRcNG7lnmFCiKKri+P0kcgX+bLckst98j6Ypf1ExS5LW0e7756jG/tmXXsu4eZtpuHUP4+ZtTp2wG9mlmPXcc0SgaJk66qq7fQ4MM4LDq55+dVVOj/6ted+t3Pi7k1MRrWQYSTJYPfdA4i4iLwJvAAeBflXtEJHRwIPAJOBFYKaqvjbY+5i4J4RPka2OyoHSKH3Yttd5bbiueoXAvD2VTVXmmzXvvJ7PrHl7eH0xjAwzmLiHYZb5sKqeWfYBc4EnVPVk4AnnuREnfk0tPjN1Q09kqSfL9PSZpRvNvD3wxWe48XcnFyqpxjCGSxQ298uA+5zH9wEzIvgMw4uNS+hfdn2FPbt/2fXuAu9TZEOPygmQZRpVhFCR6qcYxSCouCvwUxFZKyKznLYTVHUgu2UHcELAzzDqYN9Pbq6IOAFoOriffT9xEU6fIht6bZMAIaZR1FkpWv0UoxgEFfdzVfX9wIXAdSLyp+Uvasmg72rUF5FZItIlIl27dtUmchjD46ieHf7bfYpsJLVNqswtfsNNo+hL0eqnGMUgUJy7qnY7/3eKyI+Ac4CXReREVd0uIicCOz32XQQsgpJDNUg/jCNsO3QcExpq09e3HToOVyOMjzj+NK3mE0Vfilg/xcg/wxZ3ETkaaFDVN5zHFwDzgeXAVUCn839ZGB01/LG45S+Z01dbFmBxy18yL8D7pimRxW9f/IaJWv0UI48EMcucAKwSkd8ATwMrVPXfKYn6+SLyHPBnznMjJs68eBY366yKsgA36yzOvHjW0DvniHrs6Hmq4W0YAwx75K6qzwNnuLS/AkwL0ilj+JRGpp/nE49NS9yEkiT1FFVLk9nJMMLCasvkkDSZUJKiXju6HTMjb5i4G4kQ9SIjZkc3io4VDjNiJ464crOjG0XHxN2InTjiyq0OuVF0zCxjxE5cceVmRzeKjI3cjdiJooSAYRiVmLgbsWP2cMOIHjPLGLFjceWGET0m7kYimD3cMKLFzDKGYRg5xMTdMAwjh5i4G4Zh5BATd8MwjBxi4m4YhpFDpLQSXsKdENkFvBTDR40BapcpKjZ2TNyx4+KOHRd3kjoub1fVsW4vpELc40JEulS1I+l+pAk7Ju7YcXHHjos7aTwuZpYxDMPIISbuhmEYOaRo4r4o6Q6kEDsm7thxcceOizupOy6FsrkbhmEUhaKN3A3DMAqBibthGEYOyZW4i8hoEXlcRJ5z/h/rsd2/i8geEXm0qn2yiPxaRDaLyIMi0hJPz6OljuNylbPNcyJyVVn7kyKySUQ2OH/Hx9f78BGRjzjfZ7OIzHV5fYRz/jc718Okstducto3icj0WDseMcM9LiIySUR6yq6Pb8be+YjwcUz+VETWiUi/iFxR9Zrr7yk2VDU3f8ACYK7zeC5wu8d204BLgEer2pcAVzqPvwn8z6S/U1zHBRgNPO/8P9Z5fKzz2pNAR9LfI6Rj0Qj8AXgH0AL8Bnhv1TafB77pPL4SeNB5/F5n+xHAZOd9GpP+Tik4LpOAZ5L+Dgkdk0nA6cD9wBVl7Z6/p7j+cjVyBy4D7nMe3wfMcNtIVZ8A3ihvExEBzgMeGmr/DOLnuEwHHlfVV1X1NeBx4CPxdC9WzgE2q+rzqtoLPEDp+JRTfrweAqY518dlwAOqekBVXwA2O++XB4Icl7wy5DFR1RdVdSNwqGrfxH9PeRP3E1R1u/N4B3BCHfseB+xR1X7n+VYgL6tJ+Dku7cCWsufV3///OFPu/5XxH/RQ37NiG+d62Evp+vCzb1YJclwAJovIehH5hYj8j6g7GxNBznfi10rmVmISkZ8B41xe+kr5E1VVESlMnGfEx+UvVLVbRN4GPAx8mtI01DAAtgMnqeorInIWsFRETlXV15PuWJHJnLir6p95vSYiL4vIiaq6XUROBHbW8davAG0i0uSMSiYA3QG7GxshHJdu4ENlzydQsrWjqt3O/zdE5HuUpqtZFfduYGLZc7fzPLDNVhFpAkZRuj787JtVhn1ctGRkPgCgqmtF5A/Au4GuyHsdLUHOt+fvKS7yZpZZDgx4pa8Clvnd0blAfw4MeLzr2j/l+DkujwEXiMixTjTNBcBjItIkImMARKQZ+CjwTAx9joo1wMlOZFQLJcfg8qptyo/XFcBK5/pYDlzpRI1MBk4Gno6p31Ez7OMiImNFpBFARN5B6bg8H1O/o8TPMfHC9fcUUT/dSdojHbJ3+zjgCeA54GfAaKe9A1hctt1/ALuAHkq2sOlO+zso/Vg3Az8ARiT9nWI+Ln/lfPfNwGedtqOBtcBG4FngTjIeIQJcBPw/SpEQX3Ha5gOXOo+Pcs7/Zud6eEfZvl9x9tsEXJj0d0nDcQEud66NDcA64JKkv0uMx+RsR0PeojS7e7Zs35rfU5x/Vn7AMAwjh+TNLGMYhmFg4m4YhpFLTNwNwzByiIm7YRhGDjFxNwzDyCEm7oZhGDnExN0wDCOH/H+Hvsfzg2BWogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
